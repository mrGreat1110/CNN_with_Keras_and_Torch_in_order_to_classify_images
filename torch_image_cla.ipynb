{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1mfuvHmzlm0",
        "outputId": "434afdec-ee8e-4156-bc89-f036b119695c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1vMCNSczpgF"
      },
      "outputs": [],
      "source": [
        "!cp '/content/drive/MyDrive/Colab_Notebooks/FN4/assingment1/image_classification.zip' '/content/image-classification.zip'\n",
        "!unzip '/content/image-classification.zip'\n",
        "!rm  '/content/image-classification.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlibm3GHzOxG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDY5p2A_zaz2",
        "outputId": "ac3dc0dc-2133-4d6a-a7d7-c5e96f218c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:174: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 10/10 [03:34<00:00, 21.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.745, 0.7946666666666666, 0.8106666666666666, 0.831, 0.8346666666666667, 0.8433333333333334, 0.8473333333333334, 0.8396666666666667, 0.8453333333333334, 0.842]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pickletools import optimize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "# import transformers\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision \n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "torch.manual_seed(6)\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "class_names_label = {'buildings': 0,\n",
        "                    'forest' : 1,\n",
        "                    'glacier' : 2,\n",
        "                    'mountain' : 3,\n",
        "                    'sea' : 4,\n",
        "                    'street' : 5\n",
        "                    }\n",
        "nb_classes = 6\n",
        "\n",
        "path_train='seg_train/seg_train'\n",
        "path_pred='seg_pred/seg_pred'\n",
        "path_test='seg_test/seg_test'\n",
        "list_sub=os.listdir(path_train)\n",
        "\n",
        "# create csv file which contain the class of image\n",
        "\n",
        "id_image=[]\n",
        "clas=[]\n",
        "location=[]\n",
        "for sub in list_sub:\n",
        "    list_ima_class=os.listdir(path_train+'/'+sub)\n",
        "    for ima in list_ima_class:\n",
        "        id_image.append(ima)\n",
        "        clas.append(sub)\n",
        "        location.append(path_train+'/'+sub+'/'+ima)\n",
        "df=pd.DataFrame({'image':id_image, 'class':clas, 'location':location})\n",
        "df['class']=df['class'].replace(class_names_label)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.to_csv('data_train.csv')\n",
        "\n",
        "\n",
        "id_image=[]\n",
        "clas=[]\n",
        "location=[]\n",
        "for sub in list_sub:\n",
        "    list_ima_class=os.listdir(path_test+'/'+sub)\n",
        "    for ima in list_ima_class:\n",
        "        id_image.append(ima)\n",
        "        clas.append(sub)\n",
        "        location.append(path_test+'/'+sub+'/'+ima)\n",
        "df2=pd.DataFrame({'image':id_image, 'class':clas, 'location':location})\n",
        "df2['class']=df2['class'].replace(class_names_label)\n",
        "df2 = df2.sample(frac=1).reset_index(drop=True)\n",
        "df2.to_csv('data_test.csv')\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, file_csv):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.df=pd.read_csv(file_csv)\n",
        "        self.name=self.df['image'].values\n",
        "        self.clas=self.df['class'].values\n",
        "        self.location=self.df['location'].values\n",
        "        self.len=len(self.clas)\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image=cv2.imread(self.location[index])\n",
        "        image=cv2.resize(image,(150,150))\n",
        "        image = np.moveaxis(image, -1, 0)\n",
        "        image=torch.tensor(image)\n",
        "        image=image.to(device)\n",
        "        y=self.clas[index]\n",
        "        y=torch.tensor(y)\n",
        "        y=y.to(device)\n",
        "        return image, y\n",
        "\n",
        "\n",
        "dataset=Dataset(file_csv='data_train.csv')\n",
        "vali_dataset=Dataset(file_csv='data_test.csv')\n",
        "# plt.imshow(dataset[3][0])\n",
        "# print(dataset[3][0].shape)\n",
        "# plt.show()\n",
        "# print(dataset)\n",
        "\n",
        "# Tao model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # l1\n",
        "        self.cv1=nn.Conv2d(in_channels=3,out_channels=8, kernel_size=3, padding='same')\n",
        "        self.a1=torch.nn.ReLU()\n",
        "        self.b1=nn.BatchNorm2d(8)\n",
        "\n",
        "        # l2\n",
        "        self.cv2=nn.Conv2d(in_channels=8,out_channels=16, kernel_size=3, padding='same')\n",
        "        self.a2=torch.nn.ReLU()\n",
        "        self.b2=nn.BatchNorm2d(16)\n",
        "\n",
        "        # l3\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
        "        # l4\n",
        "        self.cv3=nn.Conv2d(in_channels=16,out_channels=32, kernel_size=3, padding='same')\n",
        "        self.a3=torch.nn.ReLU()\n",
        "        self.b3=nn.BatchNorm2d(32)\n",
        "\n",
        "        \n",
        "        # l5\n",
        "        self.cv4=nn.Conv2d(in_channels=32,out_channels=64, kernel_size=3, padding='same')\n",
        "        self.a4=torch.nn.ReLU()\n",
        "        self.b4=nn.BatchNorm2d(64)\n",
        "\n",
        "        # l6\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=3)\n",
        "        # l7\n",
        "        self.cv5=nn.Conv2d(in_channels=64,out_channels=128, kernel_size=3, padding='same')\n",
        "        self.a5=torch.nn.ReLU()\n",
        "        self.b5=nn.BatchNorm2d(128)\n",
        "\n",
        "        # l8\n",
        "        self.cv6=nn.Conv2d(in_channels=128,out_channels=256, kernel_size=3, padding='same')\n",
        "        self.a6=torch.nn.ReLU()\n",
        "        self.b6=nn.BatchNorm2d(256)\n",
        "\n",
        "        # l9\n",
        "        self.maxpool3=nn.MaxPool2d(kernel_size=5)\n",
        "        # l10\n",
        "        self.flat1=nn.Flatten()\n",
        "\n",
        "        # l11\n",
        "        self.fc1=nn.LazyLinear(out_features=100)\n",
        "        self.a7=torch.nn.ReLU()\n",
        "        self.b7=nn.BatchNorm1d(100)\n",
        "        # l12\n",
        "        self.drop=nn.Dropout(p=0.5)\n",
        "        # l13\n",
        "        self.fc2=nn.LazyLinear(out_features=6)\n",
        "        self.sm=nn.Softmax()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.b1(self.a1(self.cv1(x)))\n",
        "        x=self.b2(self.a2(self.cv2(x)))\n",
        "\n",
        "        x=self.maxpool1(x)\n",
        "        x=self.b3(self.a3(self.cv3(x)))\n",
        "        # x=nn.BatchNorm2d(32)(x)\n",
        "\n",
        "        x=self.b4(self.a4(self.cv4(x)))\n",
        "        # x=nn.BatchNorm2d(64)(x)\n",
        "\n",
        "        x=self.maxpool2(x)\n",
        "        x=self.b5(self.a5(self.cv5(x)))\n",
        "        # x=nn.BatchNorm2d(128)(x)\n",
        "\n",
        "        x=self.b6(self.a6(self.cv6(x)))\n",
        "        # x=nn.BatchNorm2d(256)(x)\n",
        "\n",
        "        x=self.maxpool3(x)\n",
        "        x=self.flat1(x)\n",
        "        x=self.b7(self.a7(self.fc1(x)))\n",
        "        x=self.drop(x)\n",
        "        x=self.sm(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "model=Net()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# compile model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "lr=0.0001\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
        "train_loader=DataLoader(dataset=dataset, batch_size=100)\n",
        "validation_loader=DataLoader(dataset=vali_dataset, batch_size=100)\n",
        "\n",
        "\n",
        "n_epochs=10\n",
        "cost_list=[]\n",
        "accuracy_list=[]\n",
        "N_test=len(vali_dataset)\n",
        "COST=0\n",
        "import tqdm\n",
        "def train_model(n_epochs):\n",
        "    for epoch in tqdm.tqdm(range(n_epochs)):\n",
        "        COST=0\n",
        "        for x, y in (train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            z = model(x.float())\n",
        "            loss = criterion(z, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            COST+=loss.data\n",
        "        \n",
        "        cost_list.append(COST)\n",
        "        correct=0\n",
        "        #perform a prediction on the validation  data  \n",
        "        for x_test, y_test in validation_loader:\n",
        "            z = model(x_test.float())\n",
        "            _, yhat = torch.max(z.data, 1)\n",
        "            correct += (yhat == y_test).sum().item()\n",
        "\n",
        "        accuracy = correct / N_test\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "\n",
        "     \n",
        "train_model(n_epochs)\n",
        "\n",
        "\n",
        "print(accuracy_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAoFB9kQbipu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}